{"cells":[{"cell_type":"markdown","metadata":{"id":"_qxOAvI6h6PL"},"source":["#**ENS'IA Session 2**\n","\n","Welcome to the second AI session!  \n","Today you will code a **Neural Network**!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ob04JuJYhulH"},"outputs":[],"source":["#Some useful imports\n","import math\n","from pylab import *\n","import matplotlib.pyplot as plt\n","from random import *\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"E3k-nWfc8NME"},"source":["Last session, we took a look at the behavior of a single neuron.  \n","This time, we'll code a layer of neurons!\n","#Making the dataset\n","But first, we'll create our dataset. We'll try to make the Network learn about the unit circle!  \n","More precisely, we'll want the network to replicate the function:\n","$$x,y \\longrightarrow \n","      \\left\\{\n","        \\begin{array}{ll}\n","            1 & \\mbox{if } x^2+y^2 \\leq 1 \\\\\n","            0 & \\mbox{otherwise}\n","        \\end{array}\n","        \\right.\n","$$\n","\n","To do that, we'll make a grid of $N$x$N$ points of $[-1.5,1.5]^2$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BKziRwOo8NxP"},"outputs":[],"source":["#We are on a NxN grid.\n","N = 50\n","\n","#List of all the points in [-1.5,1.5]^2 on an evenly spaced 50x50 grid\n","X_train = np.array([(i//N - N//2,i%N - N//2) for i in range(N*N)])/(N//3)\n","\n","#We make this copy for future use\n","dots = np.copy(X_train)\n","\n","#And we prepare the outputs\n","Y_train = np.array([int(i*i+j*j <= 1) for i,j in X_train])\n","\n","X_train = X_train.reshape(N*N,2,1) #We change the shape to make it more adapted: we'll get into why in a later session"]},{"cell_type":"markdown","metadata":{"id":"0hvJOAdG-Gox"},"source":["Let's look at our dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":18482,"status":"ok","timestamp":1667853538399,"user":{"displayName":"Adeal's Music Box","userId":"04019869612316783395"},"user_tz":-60},"id":"NeduPhRD-HCx","outputId":"f716e8fe-ebaa-4f09-c841-2ac7a5c7d5c0"},"outputs":[],"source":["for i in range(len(Y_train)):\n","    if Y_train[i]:\n","        plt.plot(dots[i][0],dots[i][1],\"r.\")\n","    else:\n","        plt.plot(dots[i][0],dots[i][1],\"b.\")\n","\n","plt.axis(\"equal\") \n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"k8PZTOO3iHvA"},"source":["# Creating the Layers\n","\n","So, in order to create a layer of neurons, we'll create a class *Layer*, that can do the same things a neuron can:\n","- the forward function computes the forward value of each neuron in the layer\n","- the backward function computes the backward value of each neuron in the layer, and updates the parameters in the case of a Dense Layer.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01b-izYqj_hK"},"outputs":[],"source":["class Layer:\n","\n","  \"\"\"\n","    For the moment, we don't define any of the functions.\n","    We just set the value of the input to None.\n","  \"\"\"\n","  def __init__(self):\n","    # Reminder, in the __init__ method, we define what the object is composed of. Here it has a \"input\" field\n","    self.input = None\n","\n","  def forward(self, input):\n","    # The keyword \"pass\" means \"do nothing\". If we just write nothing, there will be an error stating that the function is not complete.\n","    # \"pass\" is the keyword that means \"this function does nothing but it is intentionnal\" \n","    pass \n","\n","  def backward(self, output_gradient, learning_rate):\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"XLg18QWAm2V_"},"source":["As we've seen in the presentation, we will split the Layer into a Dense and Sigmoid Layer.\n","\n","## Dense layers\n","Let's say we have a Dense layer with input size p and output size q.  \n","It has two attributes: a weights matrix $W = (w_{ij})_{i,j \\in [1,p]𝙭[1,q]}$, and a bias vector $B = (b_i)_{i \\in [1,q]}$.\n"]},{"cell_type":"markdown","metadata":{"id":"Et_ustBng6Qh"},"source":["### Forward propagation\n","\n","if we have as an input a vector $X = (x_i)_{i \\in [1,p]}$, then the output $Y = (y_i)_{i \\in [1,q]}$ would be :  \n","$$\\begin{align} \\forall i \\in [1,q],\\ \\  y_i &= \\sum_{j=0}^q w_{ij}x_j + b_i \\\\\n","\\end{align}$$\n","\n","Therefore, we get:  $$Y = W.X + B$$\n","Which can be written elegantly with numpy functions.  \n","Refer to the annex at the bottom of this notebook for some useful numpy functions !"]},{"cell_type":"markdown","metadata":{"id":"KXN9M2xig3XH"},"source":["### Backpropagation\n","\n","We receive $\\frac{\\partial L}{\\partial Y}$, the gradient, from the next layer, and we want to compute:\n","- $\\frac{\\partial L}{\\partial W} = (\\frac{\\partial L}{\\partial w_{ij}})_{i,j}$ : to update the weights\n","- $\\frac{\\partial L}{\\partial B} = (\\frac{\\partial L}{\\partial b_i})_i$ : to update the biases\n","- $\\frac{\\partial L}{\\partial X} = (\\frac{\\partial L}{\\partial x_i})_i$ : to give to the previous layer (i.e. the output of the function)\n","\n","Here's the formula for each one:\n","$$\\begin{align}\n","&\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Y}.X^T \\\\ \\\\\n","&\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial Y} \\\\ \\\\\n","&\\frac{\\partial L}{\\partial X} = W^T.\\frac{\\partial L}{\\partial Y}\n","\\end{align}$$  \n","These can also be elegantly computed using numpy functions !  **Do not try to do it manually !**\n","\n","As a reminder, when updating the parameters, we do:  \n","$$W = W -\\eta \\frac{\\partial L}{\\partial W}$$  \n","$$B = B -\\eta \\frac{\\partial L}{\\partial B}$$\n","\n","The proof for finding each formula is detailled below :\n","- Using the chain rule :\n","$$\\forall i,j , \\ \\ \\frac{\\partial L}{\\partial w_{ij}} = \\sum_{k=0}^q\\frac{\\partial L}{\\partial y_k}\\frac{\\partial y_k}{\\partial w_{ij}}$$\n","But : $$ y_k = \\sum_{j=0}^pw_{kj}x_j + b_k$$\n","Therefore : $$\\frac{\\partial y_k}{\\partial w_{ij}} =  \n","\\begin{cases}\n"," x_j \\text{ if } i = k \\\\\n"," 0 \\text{ otherwise}\n","\\end{cases}$$\n","And thus we get: $$\\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial y_i}\\frac{\\partial y_i}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial y_i}x_j$$  \n","Finally : $$\\frac{\\partial L}{\\partial W}  = \\frac{\\partial L}{\\partial Y}.X^T$$  \n","\n","\n","- Using the same reasoning, we can deduce that:\n","$$\\forall i, \\ \\ \\frac{\\partial L}{\\partial b_i} = \\sum_{k=0}^q\\frac{\\partial L}{\\partial y_k}\\frac{\\partial y_k}{\\partial b_i}$$\n","And :\n","$$\\frac{\\partial y_k}{\\partial b_i} =  \n","\\begin{cases}\n"," 1 \\text{ if } i = k \\\\\n"," 0 \\text{ otherwise}\n","\\end{cases}$$\n","And we simply get:$$\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial Y}$$\n","\n","- Using the chain rule again:\n","$$\\forall i, \\ \\ \\frac{\\partial L}{\\partial x_i} = \\sum_{k=0}^q\\frac{\\partial L}{\\partial y_k}\\frac{\\partial y_k}{\\partial x_i}$$\n","And : $$\\frac{\\partial y_k}{\\partial x_i} =\\begin{cases}\n"," w_{ki} \\text{ if } i = k \\\\\n"," 0 \\text{ otherwise}\n","\\end{cases}$$\n","Therefore :\n","$$ \\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial y_i}\\frac{\\partial y_i}{\\partial x_i}$$\n","which translates to :\n","$$\\frac{\\partial L}{\\partial X} = W^T.\\frac{\\partial L}{\\partial Y}$$\n","which is the output of the backward function.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UHEdUKDm2j-"},"outputs":[],"source":["class Dense(Layer):\n","\n","  \"\"\"\n","    This class is inherited from the class \"Layer\", which means it has the same functions\n","    and attributes as the class \"Layer\".\n","    This layer has a weights matrix and a biases vector.\n","  \"\"\"\n","  def __init__(self, input_size, output_size):\n","    #TODO : find the sizes for the weights and bias matrix\n","    self.weights = np.random.randn(..., ...)\n","    self.bias = np.random.randn(..., ...)\n","\n","  \"\"\"\n","    Computes the output vector\n","    This function must also set the value of self.input\n","  \"\"\"\n","  def forward(self, input):\n","    self.input = input\n","\n","    #TODO\n","    ...\n","\n","  \"\"\"\n","    Updates the weights matrix and the bias vector,\n","    Then returns the gradient of the error in with respect to the input\n","  \"\"\"\n","  def backward(self, output_gradient):\n","    learning_rate = 0.1 #fixed value\n","\n","    #TODO\n","    ..."]},{"cell_type":"markdown","metadata":{"id":"mOEeTovnzGvO"},"source":["Let's test it !"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mCcbmPeIzH5J"},"outputs":[],"source":["test_layer = Dense(2,4)\n","\n","#setting concrete values\n","test_layer.weights = np.array([[1., 2.], [3., 4.], [5., 6.], [7., 8.]])\n","test_layer.bias = np.array([[-1.], [2.], [-3.], [4.]])\n","\n","input = np.array([[-1], [2]])\n","expected_output = np.array([[2], [7], [4], [13]])\n","\n","#forward test\n","\n","print(\"output of the layer :\\n\",test_layer.forward(input))\n","np.testing.assert_almost_equal(expected_output,test_layer.forward(input),3)\n","\n","\n","\n","output_gradient = np.array([[0], [1], [0], [-1]])\n","expected_gradient = np.array([[-3.8], [-4.4]])\n","expected_weights = np.array([[1., 2.], [3.1, 3.8], [5., 6.], [6.9, 8.2]])\n","expected_bias = np.array([[-1], [1.9], [-3.], [4.1]])\n","\n","#backward test\n","obtained_gradient = test_layer.backward(output_gradient)\n","print(\"dL/dX computed by the layer :\\n\", obtained_gradient)\n","print(\"updated weights :\\n\", test_layer.weights)\n","print(\"updated bias :\\n\", test_layer.bias)\n","\n","np.testing.assert_almost_equal(obtained_gradient,expected_gradient,3)\n","np.testing.assert_almost_equal(test_layer.weights, expected_weights, 3)\n","np.testing.assert_almost_equal(test_layer.bias, expected_bias, 3)\n"]},{"cell_type":"markdown","metadata":{"id":"YehiPRn1-pux"},"source":["## Sigmoid Layer\n","For the Sigmoid layer, we won't need any special attributes."]},{"cell_type":"markdown","metadata":{"id":"NnRe9QRdgtC8"},"source":["### Forward propagation\n","\n","if we have as an input a vector $X = (x_i)_{i \\in [1,p]}$, then the output $Y = (y_i)_{i \\in [1,q]}$ would be : \n","\n","$$\\forall i \\in [1,q], \\ \\ y_i = \\sigma(x_i)$$\n","\n","We get:  $$Y = \\sigma(X)$$  "]},{"cell_type":"markdown","metadata":{"id":"M709baErgvxK"},"source":["### Backpropagation\n","\n","Since we don't need to update any parameters, we'll just need to compute :\n","- $\\frac{\\partial L}{\\partial X}$ : to give to the previous layer (the output of this function)\n","\n","Here's the formula :\n","\n","$$ \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y}\\sigma'(X)$$  \n","**Note that this is NOT a matrix multiplication, but the result of multiplying each coefficients one on one.**\n","\n","The proof for finding the formula is detailled below :\n","- Using the chain rule :\n","$$\\forall i, \\ \\ \\frac{\\partial L}{\\partial x_i} = \\sum_{k=0}^q\\frac{\\partial L}{\\partial y_k}\\frac{\\partial y_k}{\\partial x_i}$$\n","But : $$ y_k = \\sigma(x_k)$$\n","Therefore : $$\\frac{\\partial y_k}{\\partial x_i} =  \n","\\begin{cases}\n"," \\sigma'(x_i) \\text{ if } i = k \\\\\n"," 0 \\text{ otherwise}\n","\\end{cases}$$\n","And thus we get: $$\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial y_i}\\frac{\\partial y_i}{\\partial x_i} = \\frac{\\partial L}{\\partial y_i}\\sigma'(x_i)$$  \n","Finally : $$\\frac{\\partial L}{\\partial X}  = \\frac{\\partial L}{\\partial Y}\\sigma'(X)$$  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"shnXsArw-qKU"},"outputs":[],"source":["def sigmoid(valeur):\n","  return(1 / (1 + exp( - valeur )))\n","\n","def sigmoid_derivative(valeur):\n","  return sigmoid(valeur)*(1-sigmoid(valeur))\n","\n","class Sigmoid(Layer):\n","  \"\"\"\n","    This class also inherits from \"Layer\"\n","  \"\"\"\n","\n","  \"\"\"\n","    Computes and returns the output vector\n","    This function must also set the value of self.input\n","  \"\"\"\n","  def forward(self, input):\n","    self.input = input\n","\n","    #TODO\n","    ...\n","\n","  \"\"\"\n","    Computes and returns the gradient of the error in with respect to the input\n","  \"\"\"\n","  def backward(self, output_gradient):\n","\n","    #TODO\n","    ..."]},{"cell_type":"markdown","metadata":{"id":"ub3y-1tk13Ar"},"source":["Let's test it !"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ucO8mx-613q3"},"outputs":[],"source":["test_layer = Sigmoid()\n","\n","input = np.array([[1], [2], [3], [4]])\n","expected_output = np.array([[0.73105858], [0.88079708], [0.95257413], [0.98201379]])\n","\n","#forward test\n","print(\"output of the layer :\\n\",test_layer.forward(input))\n","np.testing.assert_almost_equal(expected_output,test_layer.forward(input),3)\n","\n","\n","\n","output_gradient = np.array([[0], [1], [0], [-1]])\n","expected_gradient = np.array([[0.], [0.10499359], [0.], [-0.01766271]])\n","\n","#backward test\n","\n","obtained_gradient = test_layer.backward(output_gradient)\n","print(\"dL/dX computed by the layer :\\n\", obtained_gradient)\n","np.testing.assert_almost_equal(obtained_gradient,expected_gradient,3)"]},{"cell_type":"markdown","metadata":{"id":"t27F7u5SgVzq"},"source":["# Creating the Neural Network"]},{"cell_type":"markdown","metadata":{"id":"Og6FnIIEgYhH"},"source":["## Forward propagation\n","\n","The input fed into the neural network is passed through the first layer, then propagated to the next layer and so on until the last layer.  \n","This last output is the output of the network.\n"]},{"cell_type":"markdown","metadata":{"id":"pyRNmz3__H35"},"source":["## Backpropagation\n","\n","We first need to calculate the derivative of the loss with respect to the output.  \n","The Loss is calculated by the MSE (Mean Square Error): $L=\\frac{1}{n}\\sum_{k =1}^n(\\hat{y_k} - y_k)^2$.  \n","By derivating : $\\forall i \\in [1,q], \\ \\ \\frac{\\partial L}{\\partial y_i} = \\frac 2 n (\\hat y_i - y_i)$  \n","Therefore $$\\frac{\\partial L}{\\partial Y} = \\frac 2 n (\\hat Y - Y)$$\n","\n","This is the gradient we'll feed into the last layer, then propagated to the previous layer and so on until the first layer, updating the parameters for each layer.\n"]},{"cell_type":"markdown","metadata":{"id":"8813ZXDPgbjp"},"source":["## Training\n","\n","In order to train, we use a batch of inputs:  \n","for each input,\n","- we compute the propagation forward,\n","- we compute the loss and it's derivative with respect to the output\n","- we compute the propagation backwards\n","\n","And we loop this process for a number of epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ybv7f121_ILH"},"outputs":[],"source":["def mse_derivative(y_expected, y_predicted):\n","  \"\"\"\n","    This function calculates the derivative of the Mean Square Error\n","    with respect to the predicted values.\n","  \"\"\"\n","  return 2 * (y_predicted - y_expected) / np.size(y_expected)\n","\n","\n","\n","class NeuralNet:\n","\n","  \"\"\"\n","    This class represents the Neural Network, which really is just a list of Layers.\n","  \"\"\"\n","  def __init__(self, layers):\n","    self.layers = layers\n","\n","  \"\"\"\n","    This function propagates the input through all of the layers.\n","    The output of a layer is the input of the next one, until there are no more layers\n","    In that case, the output of the last layer is the output of the network.\n","  \"\"\"\n","  def forward(self, input):\n","    #TODO\n","    ...\n","\n","  \"\"\"\n","    This function backpropagates the gradient through each layer.\n","    The error with respect to the input of one layer is the error with respect to the output of the previous layer.\n","\n","    grad is the loss with respect to the output\n","  \"\"\"\n","  def backward(self,grad):\n","\n","    #TODO\n","    ...\n","\n","  \"\"\"\n","    This function trains the neural network.\n","  \"\"\"\n","  def train(self, input_batch, output_batch, epochs):\n","\n","    for e in range(epochs):\n","      print(\"progression: \" + str(e+1) + \"/\" + str(epochs))\n","\n","      #TODO\n","      ...\n","\n","\n","\n","  \"\"\"\n","    This function returns the answer that the network is most confident with\n","  \"\"\"\n","  def predict(self,input):\n","    prediction = self.forward(input)\n","\n","    #We return the index with the highest confidence\n","    return np.argmax(prediction), max(prediction)"]},{"cell_type":"markdown","metadata":{"id":"FGC9iP9XAUcl"},"source":["So, let's make the network !\n","Here, it is just a list of different layers.  \n","\n","we'll be making our network with the following layers:\n","- Dense of output size 10\n","- Sigmoid\n","- Dense of output size 2\n","- Sigmoid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29f2AMJzAUKG"},"outputs":[],"source":["#Define your neural network here !\n","#Make sure the output size of a layer is the same as the input size of the next :)\n","net = NeuralNet([\n","  #TODO\n","  ...\n","])\n"]},{"cell_type":"markdown","metadata":{"id":"jaimunRnAq6x"},"source":["Let's train this neural network !\n","\n","We'll choose 50 epochs.  \n","We have to perform some manipulations on the data to make it work properly, we'll go in more detail another time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jlzq1ZWeArOt"},"outputs":[],"source":["epochs = 50\n","\n","def one_hot_encode(Y):\n","  n = Y.size\n","  m = max(Y)\n","  res = np.zeros((m+1)*n).reshape(n,m+1,1)\n","  for i in range(len(Y)):\n","    res[i][Y[i]][0] = 1\n","\n","  return res\n","\n","X_batch = X_train\n","Y_batch = one_hot_encode(Y_train)\n","\n","net.train(X_batch, Y_batch, epochs)"]},{"cell_type":"markdown","metadata":{"id":"gcgDK782A649"},"source":["Finally, let's plot our data !  \n","The points outside of the circle are in blue, the points inside are in red.  \n","The points that are in the circle according to the neural netork are in yellow if they actually are in the circle, an in green if they're not."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":3326,"status":"ok","timestamp":1667597045147,"user":{"displayName":"Adeal's Music Box","userId":"04019869612316783395"},"user_tz":-60},"id":"7OBMiyORA7No","outputId":"bc041504-399b-43ac-c0ea-755b66704602"},"outputs":[],"source":["for i,j in dots:\n","  if net.predict([[i],[j]])[0]:\n","    if i*i+j*j <= 1:\n","      plt.plot(i,j,\".\", color=\"orange\")\n","    else:\n","      plt.plot(i,j,\"g.\")\n","  else:\n","    if i*i+j*j <= 1:\n","      plt.plot(i,j,\"r.\")\n","    else:\n","      plt.plot(i,j,\"b.\")\n","\n","plt.axis(\"equal\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"YLTDNeMFgCEG"},"source":["## Training vizualisation\n","This part is already completed and is just made to make you see how the model learns. You can skip the code, just run it and observe what happened."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YhfZ7rh9Zr7Z"},"outputs":[],"source":["# New neural network (we start traing from scratch)\n","net = NeuralNet([\n","  Dense(2,10),\n","  Sigmoid(),\n","  Dense(10,2),\n","  Sigmoid()\n","])\n","\n","epochs = 50\n","\n","for epoch in range(epochs):\n","  # Predict and save image\n","  res_in = [[],[]]\n","  res_out = [[],[]]\n","  res_fail_in = [[],[]]\n","  res_fail_out = [[],[]]\n","\n","\n","  for i,j in dots:\n","    if net.predict([[i],[j]])[0]:\n","      if i*i+j*j <= 1:\n","        res_in[0].append(i)\n","        res_in[1].append(j)\n","      else:\n","        res_fail_in[0].append(i)\n","        res_fail_in[1].append(j)\n","    else:\n","      if i*i+j*j <= 1:\n","        res_fail_out[0].append(i)\n","        res_fail_out[1].append(j)\n","      else:\n","        res_out[0].append(i)\n","        res_out[1].append(j)\n","\n","  plt.plot(res_in[0],res_in[1],color=\"orange\")\n","  plt.plot(res_out[0],res_out[1],\"b.\")\n","  plt.plot(res_fail_in[0],res_fail_in[1],\"g.\")\n","  plt.plot(res_fail_out[0],res_fail_out[1],\"r.\")\n","\n","  plt.axis(\"equal\")\n","  plt.savefig('backup{}.png'.format(epoch))\n","\n","  # Train\n","  net.train(X_batch, Y_batch, 1)\n","\n","\n","\n","# Last prediction\n","for i,j in dots:\n","  if net.predict([[i],[j]])[0]:\n","    if i*i+j*j <= 1:\n","      plt.plot(i,j,\".\", color=\"orange\")\n","    else:\n","      plt.plot(i,j,\"g.\")\n","  else:\n","    if i*i+j*j <= 1:\n","      plt.plot(i,j,\"r.\")\n","    else:\n","      plt.plot(i,j,\"b.\")\n","\n","plt.axis(\"equal\")\n","plt.savefig('backup{}.png'.format(epochs+1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPfvcHw1bMv4"},"outputs":[],"source":["# Create the gif\n","import imageio\n","images = []\n","for epoch in range(epochs + 1):\n","  images.append(imageio.imread('backup{}.png'.format(epoch)))\n","imageio.mimsave('animated_gif.gif', images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rT2o8_1bfNj4"},"outputs":[],"source":["# Show the results:\n","# Either click on the files button on the left panel for google colab, then double click on the gif and it will be displayed\n","# Or on you computer open the gif saved in the current directory"]},{"cell_type":"markdown","metadata":{"id":"gl_ffJEnQlzA"},"source":["# Annex: numpy functions\n","\n","Let's consider the matrix $A=(a_{ij})_{i,j \\in [1,n]}$ et $B = (b_{ij})_{i,j \\in [1,n]}$.  \n","Then we have :\n","\n","- np.dot(A,B) : Computes $A.B$  \n","- A.T : Computes $A^T$\n","- np.multiply(A,B) : Computes the matrix $(a_{ij}b_{ij})_{i,j \\in [1,n]}$"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
