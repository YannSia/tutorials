{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1eL-pPR0xI6yDPAGFBqBUg8gQT2_E-Hjk","timestamp":1634683026017}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Z_MYurUq8fLM"},"source":["# Ens'IA - Session 4: Neural network using Tenforflow and Keras\n","\n","After having seen how both a neuron and backpropagation works, it is time to do some more serious business and make an ENTIRE neural network. Of course, we won't ask you to reprogram everything from the ground up! In order to build our neural network, we are going to use the famous **[Keras](https://keras.io/) API**! It is an API that makes building neural networks very simple and efficient.\n","\n","Keras is included in the TensorFlow library through `tf.keras`, so there is no need to import it separately."]},{"cell_type":"code","metadata":{"id":"-1K2brA28_1J"},"source":["import tensorflow as tf # We import TensorFlow\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Easing into it with the unit circle"],"metadata":{"id":"0gbEeAY5Peny"}},{"cell_type":"markdown","source":["Do you remember 2 weeks ago when we built our own neural network ?\n","What about recreating it in Keras, and seeing how it performs.\n","\n","We will start by creating our datasets.\n","\n","In order to train a neural network, we need 3 datasets:\n","\n","1.   A **training set** (~80% of total data)\n","2.   A **validation set** (~10%)\n","3.   A **test set**   (~10%)\n","\n","- The **training set** will allow you to train your neural network;\n","- The **validation set** will allow you to test your neural network during the training process in order to follow its progression;\n","- We keep an additional **test set** which does not interfere with the training in any way so that you are able to test your neural network after it has been trained. "],"metadata":{"id":"T6F6ENYUPllQ"}},{"cell_type":"code","source":["N_points = 200\n","X = np.array([ (x, y) for x in np.linspace(-1.5, 1.5, N_points) for y in np.linspace(-1.5, 1.5, N_points)])\n","Y = np.array( [  (1, 0) if (int(i*i+j*j <= 1)) else (0, 1) for i,j in X]) # We perform the one-hot encoding by hand"],"metadata":{"id":"YFl7sXQYPeJO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"X shape : {X.shape}\") # We are using a lot of points\n","print(f\"Y shape : {Y.shape}\")"],"metadata":{"id":"YUlUIeNPPnlA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(?, ?, train_size=?) # 3 arguments\n","X_test, X_val, Y_test, Y_val = train_test_split(? ,? , train_size=?)"],"metadata":{"id":"tY1Ry6u4Poky"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fantastic, now it is time to build our model, layer by layer. \n","If you don't remember, this was the architecture of our model 2 weeks ago :\n","\n","- Dense ( 2 -> 10)\n","- Sigmoid\n","- Dense (10 -> 2)\n","- Sigmoid"],"metadata":{"id":"88OoKe-ZP0hA"}},{"cell_type":"markdown","source":["There are 2 ways to implement the Sigmoid layer, both are correct"],"metadata":{"id":"flBAb2UpP0nK"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow import keras"],"metadata":{"id":"jQSvuaANP8AM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create your model here\n","# See https://keras.io/layers/core/ for details on how to create those layers ;)\n","\n","model = Sequential([\n","    # Here goes some layers!\n","    # keras.layers.something...\n","])"],"metadata":{"id":"ZCOvePMmP8Y4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary() # Only works if you have specified an input shape, one way or the other (Input layer or with a parameter in the first layer)"],"metadata":{"id":"HTd5cTpBP-36"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now it is time to choose our 2 next hyper-parameters : the loss function and the optimizer.\n","\n","We are going to use the same loss as last time, the Mean-Squared Error.\n","For the optimizer, we are going to choose 'Adam'. There are more explanations about that below."],"metadata":{"id":"6lcLEb5NQK2V"}},{"cell_type":"code","source":["from tensorflow.keras import optimizers\n","from tensorflow.keras import losses\n","\n","model.compile(\n","    optimizer= ?,\n","    loss= ?,\n","    metrics=[\"accuracy\"]\n",")"],"metadata":{"id":"PT7BMKJaQPAM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And now, it is finally time : we are going to train our model.\n","We are going to keep it at 50 epochs, with a batch size of 32."],"metadata":{"id":"r8H5bHvqQWlm"}},{"cell_type":"code","source":["model.fit(?, ?, validation_data=(?, ?), epochs=?, batch_size=?)"],"metadata":{"id":"Wca5ZYOPQYNX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The training time is so much better than last time, isn't it ?\n","This is what we get when we use an optimized library, and we are not even using any hardware accelerators."],"metadata":{"id":"cGhO0zsnQdvi"}},{"cell_type":"markdown","source":["Remember that cool visualisation ? Let's see what happens during the training of our model.\n","\n","**BEFORE RUNNING THE NEXT CELL, YOU NEED TO RESET YOUR NEURAL NETWORK BY RUNNING THE CELLS BEFORE THE ONE WHERE WE CALL model.fit()**"],"metadata":{"id":"x8RqKEB0Qdx9"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import imageio\n","\n","def plot_resultats(model: keras.Model, save_fig=None):\n","    X_eval, _ = train_test_split(X, train_size=0.1) # Much faster, but has holes in the picture\n","    #X_eval = X[1::4] # Looks better but takes a long time\n","    Y_eval = model.predict(X_eval)\n","    plt.clf()\n","    plt.axis(\"equal\")\n","\n","    for x, y in zip(X_eval, Y_eval):\n","        truth = ( x[0]**2 + x[1]**2) <= 1\n","        dedans = True if np.argmax(y) == 0 else False\n","        if dedans:\n","            if  truth:\n","                plt.plot(x[0],x[1],\".\", color=\"orange\")\n","            else:\n","                plt.plot(x[0],x[1],\"g.\")\n","        else:\n","            if truth:\n","                plt.plot(x[0],x[1],\"r.\")\n","            else:\n","                plt.plot(x[0],x[1], \"b.\")\n","\n","    if save_fig is not None:\n","        plt.title(f\"Epoch {save_fig}\")\n","        plt.savefig(f'backup{save_fig}.png')\n","\n","for i in range(20):\n","    print(f\"Epoch {i}\")\n","    if (i != 0):\n","        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=1, batch_size=32)\n","    plot_resultats(model, save_fig=i)\n","\n","images = [ imageio.imread(f'backup{i}.png') for i in range(20)]\n","imageio.mimsave('animated_gif.gif', images, duration=0.4)\n"],"metadata":{"id":"kOOm8YgCQgBA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have managed to build a very simple neural network, let's try to solve a more interesting problem."],"metadata":{"id":"jAhqlpxKRId6"}},{"cell_type":"markdown","source":["# The MNIST Dataset"],"metadata":{"id":"KCWFpCuGQmzR"}},{"cell_type":"markdown","source":["We will use the dataset called [Mnist](https://www.tensorflow.org/datasets/catalog/mnist), which is a collection of handwritten digits images widely used in Neural Network tutorials.\n"],"metadata":{"id":"4YWB8IFMQuU9"}},{"cell_type":"markdown","metadata":{"id":"fQ4m9VfZ9VSD"},"source":["In order to train a neural network, we need 3 datasets:\n","\n","1.   A **training set** (80% of total data)\n","2.   A **validation set** (~10%)\n","3.   A **test set**   (~10%)\n","\n","- The **training set** will allow you to train your neural network;\n","- The **validation set** will allow you to test your neural network during the training process in order to follow its progression;\n","- We keep an additional **test set** which does not interfere with the training in any way so that you are able to test your neural network after it has been trained."]},{"cell_type":"code","metadata":{"id":"9g17nZJ39vtm"},"source":["# Here, we load the training and test sets\n","# The validation set will have to be created from the training set (we can take 10% of the training set later for that)\n","mnist = tf.keras.datasets.mnist\n","(raw_x_train, raw_y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Taille des jeux de donnÃ©es\n","print(f\"{raw_x_train.shape}\") # The training set is composed of 60 000 images of size 28x28\n","print(f\"{x_test.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gVlrGvTU-TN6"},"source":["# Let's plot an image\n","plt.figure()\n","plt.imshow(raw_x_train[0])\n","plt.colorbar()\n","plt.grid(False)\n","plt.show()\n","# And print its class (expected output)\n","print(f\"Class: {raw_y_train[0]}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5s_hpJnR-dNo"},"source":["In order to create our validation set, we will divide the training set into 2 part (90% training, 10% validation). To do that, we will use a function from the **scikit-learn** library that we already used during the first session. Go search in its API how the `train_test_split()` function works!\n"]},{"cell_type":"code","metadata":{"id":"RU4ylVci-cu8"},"source":["from sklearn.model_selection import train_test_split\n","\n","x_train, x_val, y_train, y_val = train_test_split(?, ?, test_size=?) # 3 arguments"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nc-EmqSbKz4A"},"source":["x_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vx3LBHeHHrTn"},"source":["Do you remember the neural network input? It was under the form of a 1 dimensional vector, so we need to resize all the images (flatten) from (28, 28) to (28 * 28,) = (784,)"]},{"cell_type":"code","metadata":{"id":"Pami-5W7IMU7"},"source":["x_train = x_train.reshape(?)\n","x_val = x_val.reshape(?)\n","x_test = x_test.reshape(?)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-1CM5SZ18fSS"},"source":["And the output? It should the a 1D vector representing the classes (labels) under the one hot encoding form. So we use the `to_categorical()` **Keras** function to transform our classes (0, 1, 2, ..., 9) into one hot encoding."]},{"cell_type":"code","metadata":{"id":"sjZJqF9g8fe1"},"source":["y_train = tf.keras.utils.to_categorical(?)\n","y_val = tf.keras.utils.to_categorical(?)\n","\n","y_test_no_onehot = y_test # We keep this here for the confusion matrix at the end ;)\n","y_test = tf.keras.utils.to_categorical(?)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RNFe3X8U_C9I"},"source":["We now have 3 datasets: train, val and test.\n","Now comes the model creation part!\n","\n","To do that, we are going to create a neural network that takes our images as input and gives us the correct number in output.\n","So we need an input of size equal to the number of pixels in an image (28x28).\n","The output will have to be equal to the number of classes (one hot encoding: refer to the slides or ask for help if you did not understand!; 10 because we have 10 numbers).\n","\n","You will create 3 layers:\n","* A **dense** layer with **128 neurons** (which takes 28x28 vectors as input)\n","* A **dense** layer with **64 neurons**\n","* An output layer, also **dense** with **10 neurons**\n","\n","You will use the **sigmoid activation function for each dense layer**.\n","`model.summary()` will allow you to see more clearly with additional details your neural network."]},{"cell_type":"code","metadata":{"id":"fhiXYp266uOq"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow import keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UB3GbKvm_png"},"source":["# Create your model here\n","# See https://keras.io/layers/core/ for details on how to create those layers ;)\n","\n","model = Sequential([\n","    # Here goes some layers!\n","    # keras.layers.something...\n","])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4TeI6P-kAgtm"},"source":["You have now created a neural network! You now have to specify which loss function and which backpropagation method you want to use.\n","There are many of them but for this time, we will simply use **SGD** (Stockastic Gradient Descent), **categorical crossentropy** as the loss function and the **accuracy** as an additional metric that we want to display during the training."]},{"cell_type":"code","metadata":{"id":"zi5rS-N7Ad-b"},"source":["from tensorflow.keras import optimizers\n","from tensorflow.keras import losses\n","\n","model.compile(\n","    optimizer=?,\n","    loss=?, # you can also try \"mse\"\n","    metrics=[?]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oQjFflsxBHsE"},"source":["Everything is now ready to begin the training! Try to train your model on **10 epochs** with a **batch size of 64**.\n","The bravest ones can also try to specify a learning rate (the default learning rates are pretty good though!)."]},{"cell_type":"code","metadata":{"id":"2m157Rj2BGje"},"source":["import time\n","start = time.time()\n","\n","history = model.fit(\n","    ?, ?, validation_data=(?, ?),\n","    epochs=?,\n","    batch_size=?\n",")\n","\n","print(f\"Training done in {time.time() - start :.0f} seconds!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yY90y-v9CauE"},"source":["You can see your neural network make some progress after each step!\n","So you see the loss, the accuracy on the training set, along with both the loss and the accuracy on the validation set.\n","\n","The following code allows to plot the evolution of the accuracy and the loss over time. It is always useful to look at the to see how your neural network evolves."]},{"cell_type":"code","metadata":{"id":"10ecz7aHC21_"},"source":["def plot_history(history):\n","    \"\"\"\n","    Plots the accuracy and the loss\n","    \"\"\"\n","    plt.plot(history.history['accuracy'])\n","    plt.plot(history.history['val_accuracy'])\n","    plt.title('Model accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(['train', 'val'], loc='upper left')\n","    plt.show()\n","    \n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['train', 'val'], loc='upper left')\n","    plt.show()\n","\n","plot_history(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dg5gPahFDF7u"},"source":["If you see the loss decrease and the accuracy increase, it is usually good!\n","It is now time to test your neural network on the test dataset by using the `model.evaluate()` method."]},{"cell_type":"code","metadata":{"id":"y9xcrH1uDRTH"},"source":["# Evaluates your neural network on the test dataset\n","# Result: [loss, accuracy] (see https://keras.io/api/models/model_training_apis/#evaluate-method)\n","model.evaluate(?, ?) # Try to print it!\n","\n","res = model.predict(x_test) # We ask you neural network to predict on the inputs\n","print(f\"Sample prediction -> {res[0]}\") # We look at the first prediction\n","\n","# The output is actually an array of size 10 with each value corresponding to\n","# the probability that the given image is of the class corresponding to the array index\n","# In res[0], the index having the highest value is the index 7, so the predicted class is 7\n","res = np.argmax(res, axis=1) # Argmax gives the index having the highest value (so here we have 7 for res[0])\n","\n","# Let's look at the confusion matrix using sklearn!\n","from sklearn.metrics import confusion_matrix\n","print(confusion_matrix(?, ?, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SG8s2zNNrStQ"},"source":["## To go further\n","\n","Your neural network should now work quite well but in reality, there are a few problems that may not have appeared here but that may appear depending on the initialization of weights and biases, the dataset, the architecture or other things.\n","\n","### Activation functions\n","\n","Until now, each layer had the sigmoid activation function $(\\frac{1}{(1+\\exp^{-x})})$\n","![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)\n","\n","Actually, there are others that work better. The problem with **sigmoid** is that when we do the gradient descent, we will calculate the partial derivatives in order to find where to modify the weights of our neurons. However, the derivative of sigmoid for values that are a little too large or too small will be equal to 0... So we end up with a gradient of 0 and our neural network will not train anymore. The same problem occurs with another activation function, **tanh**. This problem is known as \"*vanishing gradient*\".\n","\n","Until now we used **sigmoid** but there are many that work much better than this one.\n","\n","The most commonly used is **ReLU** (Rectified Linear Unit) which function is $ReLU(x) = \\max(0, x)$, so the curve is:\n","<img src=https://cdn-images-1.medium.com/max/1600/1*DfMRHwxY1gyyDmrIAd-gjQ.png width=\"500\">\n","\n","Why do we use Relu?\n","\n","1. Inexpensive to calculate\n","2. No vanishing gradient\n","3. It is linear and converges faster\n","\n","Another famous function is **Softmax**. It is often used as the **last** activation function of a neural network.\n","There are plenty more of them, which some are derivatives of Relu; for instance, **Leaky ReLU** ou **ELU** but in practice, Relu should be enough.\n","\n","If you really want to go deeper and increase your knowledge on the activation function, go watch the 30 first minutes of https://youtu.be/wEoyxE0GP2M.\n"]},{"cell_type":"markdown","metadata":{"id":"_vgiYLDUkpff"},"source":["## The gradient descent\n","\n","![](https://blog.paperspace.com/content/images/2018/05/68747470733a2f2f707669676965722e6769746875622e696f2f6d656469612f696d672f70617274312f6772616469656e745f64657363656e742e676966.gif)\n","\n","Here is what happens in the case of a function in 3 dimensions. Of course, our neural network having many more parameters (sometimes several hundreds of thousands), the dimension is a bit higher.\n","Moreover, in this case, we can see that everything goes well, from the starting point taken, we manage to find the minimum of the function.\n","However, this is not always the case with the gradient descent algorithm. It often happens that we find a **LOCAL** minimum when we are really looking for the **global** minimum of the function. This is why other algorithms have been developed. The first idea was to add a momentum to the gradient descent: in other words, we add speed to our gradient descent, so we find the minimum more quickly.\n","\n","![](https://media.giphy.com/media/SJVFO3IcVC0M0/giphy.gif)\n","\n","Here is how different algorithms perform to find the minimum of a function. We can see that SGD (the basic gradient descent) is the slowest. **RMSProp** on the other hand, works quite well.\n","\n","The most used of them is **Adam**. One of the characteristics of Adam is that there is **one learning rate per parameter**. Moreover, it is very easy to configure (most of the time, the default parameters will give you the best possible result)\n","\n","![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/Comparison-of-Adam-to-Other-Optimization-Algorithms-Training-a-Multilayer-Perceptron.png)\n","\n","Here is a comparison of the most popular optimizers. We can see that Adam performs the best. If you want to know everything about Adam and you have some time, [you can read the article here](https://arxiv.org/pdf/1412.6980.pdf). Or if you need more explanations, [check out this video](https://youtu.be/_JB0AO7QxSA) (the first part of the course is about optimizers)."]},{"cell_type":"markdown","metadata":{"id":"tSdNutwVlCUb"},"source":["## The dropout\n","\n","One problem that can happen with your current neural network is that it suffers from **overfitting**. In other words, instead of learning to generalize well on data it has not yet seen (the test set), your neural network will learn the features of the training set and will underperform in the test phase. Imagine that we train a neural network to recognize cats and that on each of the images of the training set, the neural network sees the ears, the tail and the legs. If in the test game it only sees the ears and the paws, it might make a mistake and not identify the image as a cat.\n","\n","For this, you can add some dropout. In other words, between 2 layers, we will \"cut out\" some connections. In other words, some neurons of layer $k$ will not send their output to layer $k_{+1}$."]},{"cell_type":"code","metadata":{"id":"ha99Gk73mh99"},"source":["# Now that you've seen all this, you should be able to build a neural network\n","# with the following characteristics:\n","# - For each layer, the activation function should be ReLU except for the last\n","#   one which will use Softmax\n","# - The optimizer should be Adam instead of SGD\n","# - Uses some dropout between each layer\n","\n","model2 = keras.models.Sequential([\n","    # Here goes some layers!\n","])\n","\n","model2.compile() # And here some params!\n","\n","history = model2.fit() # Here too :3\n","\n","model2.evaluate(x_test, y_test) # Your score"],"execution_count":null,"outputs":[]}]}